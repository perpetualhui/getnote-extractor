"""
Get ç¬”è®°å†…å®¹æå–å·¥å…· v2.0
ä¸€ä¸ªåŸºäº Playwright çš„è‡ªåŠ¨åŒ–è„šæœ¬ï¼Œç”¨äºå¯¼å‡º Get ç¬”è®°ï¼ˆbiji.comï¼‰çŸ¥è¯†åº“ä¸­çš„æ‰€æœ‰æ–‡ç« 

ä¸»è¦åŠŸèƒ½ï¼š
- å¹¶è¡Œæå–ï¼šæ”¯æŒå¤šçº¿ç¨‹å¹¶å‘æå–ï¼Œé»˜è®¤3ä¸ªå¹¶å‘ï¼Œé€Ÿåº¦æå‡3-5å€
- æ–­ç‚¹ç»­ä¼ ï¼šè‡ªåŠ¨è·³è¿‡å·²æå–çš„æ–‡ç« 
- è‡ªåŠ¨åˆ†é¡µï¼šè‡ªåŠ¨å¤„ç†æ‰€æœ‰é¡µé¢
- æ™ºèƒ½å‘½åï¼šä½¿ç”¨åšä¸»åç§°å‘½åæ–‡ä»¶å¤¹ï¼Œè‡ªåŠ¨å¤„ç†é‡åå†²çª
- ä¼˜é›…åœæ­¢ï¼šCtrl+C ç­‰å¾…å½“å‰æ‰¹æ¬¡å®Œæˆåå®‰å…¨é€€å‡º
- å®æ—¶ç»Ÿè®¡ï¼šæ˜¾ç¤ºæå–è¿›åº¦ã€é€Ÿåº¦å’Œç”¨æ—¶ç»Ÿè®¡
"""

from playwright.sync_api import sync_playwright
import os
import sys
import platform
import json
import time
import re
from urllib.parse import urlparse, parse_qs
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock
import signal

# å…¨å±€å˜é‡ç”¨äºä¼˜é›…åœæ­¢
should_stop = False
stats_lock = Lock()


def get_system_chrome_path():
    """è‡ªåŠ¨æŸ¥æ‰¾ç³»ç»Ÿ Chrome æµè§ˆå™¨è·¯å¾„"""
    system = platform.system()

    if system == "Windows":
        possible_paths = [
            r"C:\Program Files\Google\Chrome\Application\chrome.exe",
            r"C:\Program Files (x86)\Google\Chrome\Application\chrome.exe",
            os.path.expanduser(r"~\AppData\Local\Google\Chrome\Application\chrome.exe")
        ]
        try:
            import winreg
            key = winreg.OpenKey(winreg.HKEY_LOCAL_MACHINE, r"SOFTWARE\Microsoft\Windows\CurrentVersion\App Paths\chrome.exe")
            path = winreg.QueryValue(key, None)
            winreg.CloseKey(key)
            if os.path.exists(path):
                return path
        except Exception:
            pass
    elif system == "Darwin":  # macOS
        possible_paths = [
            "/Applications/Google Chrome.app/Contents/MacOS/Google Chrome"
        ]
    else:  # Linux
        possible_paths = [
            "/usr/bin/google-chrome",
            "/usr/bin/chromium-browser",
            "/snap/bin/chromium"
        ]

    for path in possible_paths:
        if os.path.exists(path):
            return path

    raise FileNotFoundError(
        "æœªæ‰¾åˆ°ç³»ç»Ÿä¸­çš„ Chrome æµè§ˆå™¨ï¼Œè¯·ç¡®è®¤å·²å®‰è£…ï¼\n"
        "è®¿é—® https://www.google.com/chrome/ ä¸‹è½½å®‰è£…"
    )


def load_config(config_file="config.json"):
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    try:
        with open(config_file, "r", encoding="utf-8") as f:
            return json.load(f)
    except FileNotFoundError:
        print(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_file}")
        print("è¯·å¤åˆ¶ config.example.json ä¸º config.json å¹¶å¡«å…¥æ‚¨çš„é…ç½®")
        sys.exit(1)
    except json.JSONDecodeError as e:
        print(f"é…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯: {e}")
        sys.exit(1)


def sanitize_filename(title):
    """æ¸…ç†æ–‡ä»¶åï¼Œç§»é™¤éæ³•å­—ç¬¦"""
    # ç§»é™¤æˆ–æ›¿æ¢ Windows/Linux ä¸å…è®¸çš„å­—ç¬¦
    title = re.sub(r'[<>:"/\\|?*#]', '', title)
    # é™åˆ¶é•¿åº¦ä¸º100ä¸ªå­—ç¬¦
    return title[:100].strip()


def extract_topic_id(url_or_id):
    """ä» URL æˆ– ID ä¸­æå–çŸ¥è¯†åº“ ID"""
    if url_or_id.startswith('http://') or url_or_id.startswith('https://'):
        match = re.search(r'subject/([^/?]+)', url_or_id)
        if match:
            return match.group(1)
    return url_or_id


def extract_follow_name(url):
    """ä» URL ä¸­æå– followName å‚æ•°"""
    try:
        parsed = urlparse(url)
        params = parse_qs(parsed.query)
        if 'followName' in params:
            return params['followName'][0]
    except Exception:
        pass
    return None


def get_unique_output_dir(base_name):
    """è·å–ä¸å†²çªçš„è¾“å‡ºç›®å½•"""
    output_dir = f"./{base_name}"
    counter = 2
    while os.path.exists(output_dir):
        output_dir = f"./{base_name}_{counter}"
        counter += 1
    return output_dir


def extract_article_content(page, detail_url):
    """æå–å•ç¯‡æ–‡ç« çš„å†…å®¹"""
    try:
        web_url = f"{detail_url}/web"
        page.goto(web_url, timeout=20000, wait_until="domcontentloaded")

        # æ™ºèƒ½ç­‰å¾…ï¼šç­‰å¾…æ®µè½å‡ºç°å³å¯
        try:
            page.wait_for_selector('p', timeout=5000)
        except Exception:
            pass

        # æå–å†…å®¹
        paragraphs = page.query_selector_all('p')
        title = ''
        original_link = ''
        content_parts = []

        for p in paragraphs:
            try:
                text = p.inner_text().strip()
                if text.startswith('åŸé“¾æ¥ï¼š'):
                    original_link = text.replace('åŸé“¾æ¥ï¼š', '')
                elif not title and text and len(text) < 200:
                    title = text
                elif len(text) > 50:
                    content_parts.append(text)
            except Exception:
                continue

        main_content = '\n\n'.join(content_parts)

        return {
            'title': title,
            'original_link': original_link,
            'main_content': main_content
        }
    except Exception as e:
        print(f"  âœ— æå–å¤±è´¥: {str(e)}")
        return None


def process_article(context, article, output_dir, global_index, start_time, stats):
    """å¤„ç†å•ç¯‡æ–‡ç« """
    filename = f"{str(global_index).padStart(3, '0')}_{sanitize_filename(article['title'])}.md"
    filepath = os.path.join(output_dir, filename)

    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨ï¼ˆæ–­ç‚¹ç»­ä¼ ï¼‰
    if os.path.exists(filepath):
        print(f"  [{global_index}] â­ï¸ {article['title'][:40]}... - å·²å­˜åœ¨ï¼Œè·³è¿‡")
        with stats_lock:
            stats['skipped'] += 1
        return {'skipped': True, 'saved': False}

    # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆçš„ URL
    if not article.get('detail_url'):
        print(f"  [{global_index}] âœ— {article['title'][:40]}... - æ— æœ‰æ•ˆURL")
        return {'skipped': False, 'saved': False}

    try:
        print(f"  [{global_index}] ğŸ”„ {article['title'][:40]}...")

        # ä¸ºæ¯ç¯‡æ–‡ç« åˆ›å»ºç‹¬ç«‹çš„é¡µé¢
        page = context.new_page()
        try:
            content = extract_article_content(page, article['detail_url'])

            if content and content['main_content']:
                # ç”Ÿæˆ Markdown å†…å®¹
                markdown = f"# {article['title']}\n\n"
                markdown += f"**åŸé“¾æ¥**: {content['original_link'] or ''}\n\n"
                markdown += "---\n\n"
                markdown += content['main_content']

                with open(filepath, 'w', encoding='utf-8') as f:
                    f.write(markdown)

                with stats_lock:
                    stats['saved'] += 1

                # è®¡ç®—å¹¶æ˜¾ç¤ºå®æ—¶ç»Ÿè®¡
                elapsed_minutes = (time.time() - start_time) / 60
                avg_speed = stats['saved'] / elapsed_minutes if elapsed_minutes > 0 else 0
                print(f"  [{global_index}] âœ“ {article['title'][:40]}... - å·²ä¿å­˜ ({avg_speed:.1f} ç¯‡/åˆ†é’Ÿ)")

                page.close()
                return {'skipped': False, 'saved': True}
            else:
                print(f"  [{global_index}] âœ— {article['title'][:40]}... - å†…å®¹ä¸ºç©º")
                page.close()
                return {'skipped': False, 'saved': False}
        except Exception as e:
            print(f"  [{global_index}] âœ— {article['title'][:40]}... - å¤„ç†å¤±è´¥: {str(e)}")
            page.close()
            return {'skipped': False, 'saved': False}
    except Exception as e:
        print(f"  [{global_index}] âœ— {article['title'][:40]}... - åˆ›å»ºé¡µé¢å¤±è´¥: {str(e)}")
        return {'skipped': False, 'saved': False}


def fetch_article_urls(page, base_url, articles, current_page):
    """ä¸²è¡Œè·å–æ–‡ç«  URLï¼ˆé€šè¿‡ç‚¹å‡»ï¼‰"""
    urls = []

    for i, article in enumerate(articles):
        try:
            # è¿”å›åˆ—è¡¨é¡µ
            page.goto(base_url, timeout=20000, wait_until="domcontentloaded")
            page.wait_for_selector('tbody tr', timeout=5000)
            time.sleep(1)

            # å¦‚æœä¸æ˜¯ç¬¬ä¸€é¡µï¼Œéœ€è¦ç¿»é¡µ
            if current_page > 1:
                separator = '&' if '?' in base_url else '?'
                page_url = f"{base_url}{separator}page={current_page}"
                page.goto(page_url, timeout=20000, wait_until="domcontentloaded")
                page.wait_for_selector('tbody tr', timeout=5000)
                time.sleep(1)

            # ç‚¹å‡»ç¬¬ i è¡Œ
            page.evaluate(f"() => {{ const rows = document.querySelectorAll('tbody tr'); if (rows[{i}]) {{ const titleCell = rows[{i}].querySelector('td:first-child'); if (titleCell) titleCell.click(); }} }}")

            # ç­‰å¾…é¡µé¢è·³è½¬
            time.sleep(1.5)
            url = page.url
            urls.append({
                **article,
                'detail_url': url
            })
            print(f"  [{i+1}/{len(articles)}] è·å–URL: {article['title'][:30]}...")
        except Exception as e:
            print(f"  [{i+1}/{len(articles)}] è·å–URLå¤±è´¥: {str(e)}")
            urls.append({
                **article,
                'detail_url': None
            })

    return urls


def signal_handler(signum, frame):
    """ä¿¡å·å¤„ç†å™¨ï¼Œç”¨äºä¼˜é›…åœæ­¢"""
    global should_stop
    if not should_stop:
        should_stop = True
        print("\n\nâš ï¸ æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œæ­£åœ¨ä¼˜é›…é€€å‡º...")
        print("ç­‰å¾…å½“å‰æ‰¹æ¬¡å¤„ç†å®Œæˆ...")


def main():
    """ä¸»å‡½æ•°"""
    # è®¾ç½®ä¿¡å·å¤„ç†å™¨
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)

    # ä»å‘½ä»¤è¡Œå‚æ•°æˆ–é…ç½®æ–‡ä»¶è·å–å‚æ•°
    if len(sys.argv) < 2:
        print("ç”¨æ³•ï¼š")
        print("  python get_note.py <çŸ¥è¯†åº“URLæˆ–ID> [è¾“å‡ºç›®å½•] [æœ€å¤§é¡µæ•°] [æœ€å¤§æ–‡ç« æ•°] [å¹¶å‘æ•°]")
        print("")
        print("ç¤ºä¾‹ï¼š")
        print('  python get_note.py https://www.biji.com/subject/ABC123/DEFAULT?followName=åšä¸»å')
        print("  python get_note.py ABC123")
        print('  python get_note.py ABC123 "è‡ªå®šä¹‰ç›®å½•" 0 0 3')
        sys.exit(1)

    url_or_id = sys.argv[1]
    topic_id = extract_topic_id(url_or_id)

    # ç¡®å®šè¾“å‡ºç›®å½•å
    if len(sys.argv) >= 3:
        output_dir_name = sys.argv[2]
    else:
        follow_name = extract_follow_name(url_or_id)
        output_dir_name = follow_name or topic_id

    # è·å–ä¸å†²çªçš„è¾“å‡ºç›®å½•
    output_dir = get_unique_output_dir(output_dir_name)
    os.makedirs(output_dir, exist_ok=True)

    # å…¶ä»–å‚æ•°
    max_pages = int(sys.argv[3]) if len(sys.argv) >= 4 else 0
    max_articles = int(sys.argv[4]) if len(sys.argv) >= 5 else 0
    concurrency = int(sys.argv[5]) if len(sys.argv) >= 6 else 3

    # ç¡®å®šåŸºç¡€ URL
    if url_or_id.startswith('http://') or url_or_id.startswith('https://'):
        base_url = url_or_id
    else:
        base_url = f"https://www.biji.com/subject/{topic_id}/DEFAULT"

    print("=" * 50)
    print("  Get ç¬”è®°å†…å®¹æå–å·¥å…· v2.0")
    print("=" * 50)
    print(f"çŸ¥è¯†åº“ID: {topic_id}")
    print(f"è¾“å‡ºç›®å½•: {os.path.basename(output_dir)}")
    print(f"æœ€å¤§é¡µæ•°: {max_pages if max_pages > 0 else 'å…¨éƒ¨'}")
    print(f"æœ€å¤§æ–‡ç« æ•°: {max_articles if max_articles > 0 else 'å…¨éƒ¨'}")
    print(f"å¹¶å‘æ•°: {concurrency}")
    print("")

    # åŠ è½½é…ç½®
    config = load_config()
    chrome_user_data_dir = config.get("chrome_user_data_dir", "")

    if not chrome_user_data_dir:
        print("é”™è¯¯ï¼šè¯·åœ¨ config.json ä¸­é…ç½® chrome_user_data_dir")
        sys.exit(1)

    # éªŒè¯ç”¨æˆ·æ•°æ®ç›®å½•å­˜åœ¨
    if not os.path.exists(chrome_user_data_dir):
        print(f"é”™è¯¯ï¼šChrome ç”¨æˆ·æ•°æ®ç›®å½•ä¸å­˜åœ¨: {chrome_user_data_dir}")
        print("æç¤ºï¼šè¯·æ£€æŸ¥è·¯å¾„æ˜¯å¦æ­£ç¡®ï¼ˆåˆ° User Data è¿™ä¸€å±‚ï¼‰")
        sys.exit(1)

    try:
        system_chrome_path = get_system_chrome_path()
        print(f"âœ“ æ‰¾åˆ° Chrome æµè§ˆå™¨: {system_chrome_path}")
    except FileNotFoundError as e:
        print(f"âœ— {e}")
        sys.exit(1)

    # è®°å½•å¼€å§‹æ—¶é—´
    start_time = time.time()
    stats = {'saved': 0, 'skipped': 0}

    print("\nâš ï¸ è¯·ç¡®ä¿å·²å…³é—­æ‰€æœ‰ Chrome çª—å£")
    print("3 ç§’åç»§ç»­...")
    time.sleep(3)

    # å¯åŠ¨æµè§ˆå™¨
    with sync_playwright() as p:
        browser = p.chromium.launch_persistent_context(
            user_data_dir=chrome_user_data_dir,
            executable_path=system_chrome_path,
            headless=False,
            args=[
                "--start-maximized",
                "--no-sandbox",
                "--disable-blink-features=AutomationControlled"
            ],
            viewport={"width": 1920, "height": 1080},
            storage_state_persist=True
        )

        try:
            page = browser.new_page()
            current_page = 1

            while (max_pages == 0 or current_page <= max_pages) and not should_stop:
                print(f"\nå¤„ç†ç¬¬ {current_page} é¡µ...")

                try:
                    # æ„å»ºé¡µé¢ URL
                    if current_page == 1:
                        page_url = base_url
                    else:
                        separator = '&' if '?' in base_url else '?'
                        page_url = f"{base_url}{separator}page={current_page}"

                    print(f"è®¿é—®: {page_url}")
                    page.goto(page_url, timeout=30000, wait_until="domcontentloaded")
                    print("é¡µé¢å·²åŠ è½½ï¼Œç­‰å¾…å†…å®¹...")
                    time.sleep(3)

                    # ç­‰å¾…è¡¨æ ¼å‡ºç°
                    try:
                        page.wait_for_selector('tbody tr', timeout=10000)
                        print("è¡¨æ ¼å·²å‡ºç°")
                    except Exception as e:
                        print("æœªæ‰¾åˆ°è¡¨æ ¼ï¼Œæ£€æŸ¥é¡µé¢çŠ¶æ€...")
                        print(f"å½“å‰URL: {page.url()}")
                        print(f"é¡µé¢æ ‡é¢˜: {page.title()}")

                        # æ£€æŸ¥æ˜¯å¦éœ€è¦ç™»å½•
                        needs_login = page.evaluate("() => document.body.innerText.includes('ç™»å½•') || document.body.innerText.includes('è¯·å…ˆç™»å½•')")

                        if needs_login:
                            print("\nâš ï¸ éœ€è¦ç™»å½•ï¼")
                            print("=" * 50)
                            print("è¯·åœ¨æ‰“å¼€çš„æµè§ˆå™¨çª—å£ä¸­ç™»å½• Getç¬”è®°è´¦å·")
                            print("=" * 50)
                            print("")
                            print("æ­¥éª¤ï¼š")
                            print("1. æŸ¥çœ‹è‡ªåŠ¨æ‰“å¼€çš„æµè§ˆå™¨çª—å£")
                            print("2. åœ¨æµè§ˆå™¨ä¸­ç™»å½• www.biji.com")
                            print("3. ç™»å½•æˆåŠŸåï¼Œè„šæœ¬ä¼šè‡ªåŠ¨ç»§ç»­")
                            print("")
                            print("ç­‰å¾…ç™»å½•ä¸­ï¼ˆæœ€å¤š60ç§’ï¼‰...")
                            print("")

                            time.sleep(60)

                            # é‡æ–°åŠ è½½é¡µé¢
                            page.goto(base_url, timeout=30000, wait_until="domcontentloaded")
                            time.sleep(3)

                            try:
                                page.wait_for_selector('tbody tr', timeout=10000)
                                print("âœ… ç™»å½•æˆåŠŸï¼Œè¡¨æ ¼å·²å‡ºç°")
                            except Exception:
                                print("\nâŒ ç™»å½•è¶…æ—¶æˆ–å¤±è´¥")
                                print("è¯·ç¡®ä¿å·²åœ¨æµè§ˆå™¨ä¸­å®Œæˆç™»å½•ï¼Œç„¶åé‡æ–°è¿è¡Œè„šæœ¬")
                                browser.close()
                                sys.exit(1)
                        else:
                            raise e

                    # æå–æ‰€æœ‰æ–‡ç« ä¿¡æ¯
                    articles_data = page.evaluate("""() => {
                        const rows = document.querySelectorAll('tbody tr');
                        return Array.from(rows).map((row, index) => {
                            const titleCell = row.querySelector('td:first-child');
                            const title = titleCell ? titleCell.textContent.trim() : null;
                            return { title, index };
                        }).filter(item => item.title);
                    }""")

                    articles = [{'title': a['title'], 'index': a['index']} for a in articles_data]
                    print(f"æ‰¾åˆ° {len(articles)} ç¯‡æ–‡æ¡ˆ")

                    if len(articles) == 0:
                        if current_page == 1:
                            print("æ²¡æœ‰æ‰¾åˆ°æ–‡æ¡ˆï¼Œå¯èƒ½éœ€è¦ç™»å½•")
                        else:
                            print("æ²¡æœ‰æ›´å¤šæ–‡ç« äº†ï¼Œå·²åˆ°è¾¾æœ€åä¸€é¡µ")
                        break

                    # æ­¥éª¤1ï¼šä¸²è¡Œè·å–æ‰€æœ‰æ–‡ç« çš„ URL
                    print("\nğŸ“‹ æ­¥éª¤1: è·å–æ–‡ç« URL...")
                    articles_with_urls = fetch_article_urls(page, page_url, articles, current_page)

                    # æ­¥éª¤2ï¼šå¹¶è¡Œæå–æ–‡ç« å†…å®¹ï¼ˆåˆ†æ‰¹å¤„ç†ï¼‰
                    print("\nğŸ“ æ­¥éª¤2: å¹¶è¡Œæå–å†…å®¹...")

                    for i in range(0, len(articles_with_urls), concurrency):
                        if should_stop:
                            print("\nâ¸ï¸ åœæ­¢æå–ï¼Œä¿å­˜è¿›åº¦...")
                            break

                        if max_articles > 0 and stats['saved'] >= max_articles:
                            print(f"\nå·²è¾¾åˆ°æœ€å¤§æ–‡ç« æ•°é™åˆ¶ ({max_articles})ï¼Œåœæ­¢æå–")
                            break

                        batch = articles_with_urls[i:i + concurrency]

                        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†
                        with ThreadPoolExecutor(max_workers=concurrency) as executor:
                            futures = []
                            for batch_idx, article in enumerate(batch):
                                global_index = (current_page - 1) * 20 + i + batch_idx + 1
                                future = executor.submit(
                                    process_article,
                                    browser,
                                    article,
                                    output_dir,
                                    global_index,
                                    start_time,
                                    stats
                                )
                                futures.append(future)

                            for future in as_completed(futures):
                                try:
                                    future.result()
                                except Exception as e:
                                    print(f"å¤„ç†å¤±è´¥: {str(e)}")

                    if should_stop:
                        break

                    current_page += 1

                except Exception as e:
                    print(f"å¤„ç†ç¬¬ {current_page} é¡µå¤±è´¥: {str(e)}")
                    break

        finally:
            browser.close()

    # è®¡ç®—æœ€ç»ˆç»Ÿè®¡
    total_minutes = (time.time() - start_time) / 60
    final_speed = stats['saved'] / total_minutes if total_minutes > 0 else 0

    print("\n" + "=" * 50)
    print("  å®Œæˆ")
    print("=" * 50)
    print(f"æ€»å…±ä¿å­˜: {stats['saved']} ç¯‡æ–‡æ¡ˆ")
    if stats['skipped'] > 0:
        print(f"è·³è¿‡å·²å­˜åœ¨: {stats['skipped']} ç¯‡")
    print(f"æ€»ç”¨æ—¶: {total_minutes:.1f} åˆ†é’Ÿ")
    print(f"å¹³å‡é€Ÿåº¦: {final_speed:.1f} ç¯‡/åˆ†é’Ÿ")
    print(f"è¾“å‡ºç›®å½•: {output_dir}")


if __name__ == "__main__":
    main()
